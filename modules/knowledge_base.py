"""
çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ç®¡ç†ã¨å‡¦ç†ã‚’è¡Œã„ã¾ã™
"""
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import urllib.parse
import logging
from fastapi import HTTPException, UploadFile, File, Depends
from io import BytesIO
import PyPDF2
import io
import base64
import google.generativeai as genai
from PIL import Image
from .company import DEFAULT_COMPANY_NAME
from psycopg2.extensions import connection as Connection
from .database import get_db, update_usage_count
from .auth import check_usage_limits
import uuid
from pdf2image import convert_from_bytes 
from modules.config import setup_gemini
from .utils import transcribe_youtube_video, extract_text_from_html, _process_video_file, extract_text_from_pdf
import asyncio

logger = logging.getLogger(__name__)

import datetime
from datetime import datetime

model = setup_gemini()

# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®ä¿å­˜ç”¨ã‚¯ãƒ©ã‚¹
class KnowledgeBase:
    def __init__(self):
        self.data = None
        self.raw_text = ""
        self.columns = []
        self.sources = []  # ã‚½ãƒ¼ã‚¹ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚„URLï¼‰ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.url_data = []  # URLã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.url_texts = []  # URLã‹ã‚‰å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.file_data = []  # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.file_texts = []  # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.images = []    # PDFã‹ã‚‰æŠ½å‡ºã—ãŸç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        self.source_info = {}  # ã‚½ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ãªã©ï¼‰
        self.original_data = {}  # å„ã‚½ãƒ¼ã‚¹ã®å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸ {source_name: {'df': dataframe, 'text': text}}
        self.company_sources = {}  # ä¼šç¤¾ã”ã¨ã®ã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸ {company_id: [source_name1, source_name2, ...]}
        
    def get_company_data(self, company_id):
        """ä¼šç¤¾IDã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"""
        if not company_id or company_id not in self.company_sources:
            return None, "", []
            
        company_sources = self.company_sources.get(company_id, [])
        if not company_sources:
            return None, "", []
            
        # ä¼šç¤¾ã®ã‚½ãƒ¼ã‚¹ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        company_data = []
        company_text = ""
        company_columns = []
        
        for source in company_sources:
            if source in self.original_data:
                source_data = self.original_data[source]
                if 'df' in source_data and not source_data['df'].empty:
                    company_data.append(source_data['df'])
                if 'text' in source_data:
                    company_text += source_data['text'] + "\n\n"
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ
        combined_df = None
        if company_data:
            combined_df = pd.concat(company_data, ignore_index=True)
            company_columns = combined_df.columns.tolist()
            
        return combined_df, company_text, company_columns

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
knowledge_base = KnowledgeBase()

# URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
async def extract_text_from_url(url: str) -> str:
    """URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        # URLãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        if 'youtube.com' in url or 'youtu.be' in url:
            return transcribe_youtube_video(url)
        elif url.lower().endswith('.pdf'):
            return await extract_text_from_pdf(url)
        else:
            return await extract_text_from_html(url)
    except Exception as e:
        print(f"URLã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        return f"URLã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)} ===\n"
# URLã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
async def process_url(url: str, user_id: str = None, company_id: str = None, db: Connection = None):
    """URLã‚’å‡¦ç†ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹"""
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‹ã‚‰company_idã¨roleã‚’å–å¾—ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        if user_id:
            cursor = db.cursor()
            cursor.execute("SELECT company_id, role FROM users WHERE id = %s", (user_id,))
            user = cursor.fetchone()
            
            # ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„
            if user and user['role'] == 'employee':
                raise HTTPException(
                    status_code=403,
                    detail="ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
                )
                
            if user and user['company_id'] and not company_id:
                company_id = user['company_id']
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        if user_id:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            limits_check = check_usage_limits(user_id, "document_upload", db)
            
            if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                raise HTTPException(
                    status_code=403,
                    detail=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                )
        
        # URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        extracted_text = await extract_text_from_url(url)
        if extracted_text.startswith("URLã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼:"):
            raise HTTPException(
                status_code=500,
                detail=extracted_text  # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ãã®ã¾ã¾è¿”ã™
            )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
        sections = {}
        current_section = "ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"
        section_text = []
        
        for line in extracted_text.split('\n'):
            if line.startswith('=== ') and line.endswith(' ==='):
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é–‹å§‹
                if section_text:
                    sections[current_section] = section_text
                    section_text = []
                current_section = line.strip('= ')
            else:
                section_text.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
        if section_text:
            sections[current_section] = section_text
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        data = []
        for section_name, lines in sections.items():
            content = '\n'.join(lines)
            data.append({
                'section': section_name,
                'content': content,
                'source': 'URL',
                'url': url,
                'file': None  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
            })
        
        df = pd.DataFrame(data)
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆURLãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼‰
        _update_knowledge_base(df, extracted_text, is_file=False, source_name=url, company_id=company_id)
        
        # ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«URLã‚’è¿½åŠ ã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æœ‰åŠ¹çŠ¶æ…‹ã‚’è¨˜éŒ²
        if url not in knowledge_base.sources:
            knowledge_base.sources.append(url)
            # ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨˜éŒ²
            knowledge_base.source_info[url] = {
                'timestamp': datetime.now().isoformat(),
                'active': True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹
            }
        
        # æœ€æ–°ã®ä¼šç¤¾åã‚’å–å¾—
        from .company import DEFAULT_COMPANY_NAME as current_company_name
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°

        # if user_id and not limits_check.get("is_unlimited", False):
        if user_id:
            updated_limits = update_usage_count(user_id, "document_uploads_used", db)
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚½ãƒ¼ã‚¹ã‚’è¨˜éŒ²
            document_id = str(uuid.uuid4())
            cursor = db.cursor()
            cursor.execute(
                "INSERT INTO document_sources (id, name, type, page_count, content, uploaded_by, company_id, uploaded_at, active) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (document_id, url, "URL", 1, extracted_text, user_id, company_id, datetime.now().isoformat(), True)
            )
            
            # ä¼šç¤¾ã®ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
            if company_id:
                if company_id not in knowledge_base.company_sources:
                    knowledge_base.company_sources[company_id] = []
                if url not in knowledge_base.company_sources[company_id]:
                    knowledge_base.company_sources[company_id].append(url)
            db.commit()
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        active_sources = get_active_resources()
        limits_check = check_usage_limits(user_id, "document_upload", db)
        return {
            "message": f"{current_company_name}ã®æƒ…å ±ãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸï¼ˆURL: {url}ï¼‰",
            "columns": knowledge_base.columns if knowledge_base.data is not None else [],
            "preview": df.head(5).to_dict('records') if not df.empty else [],
            "total_rows": len(df),
            "sections": list(sections.keys()),
            "url": url,
            "sources": knowledge_base.sources,
            "active_sources": active_sources,
            "remaining_uploads": limits_check.get("remaining", None) if user_id else None,
            "limit_reached": not limits_check.get("allowed", True) if user_id else False
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"URLã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹é–¢æ•°
async def process_file(file: UploadFile = File(...), user_id: str = None, company_id: str = None, db: Connection = None):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹"""
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‹ã‚‰company_idã¨roleã‚’å–å¾—ï¼ˆæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
    if user_id:
        cursor = db.cursor()
        cursor.execute("SELECT company_id, role FROM users WHERE id = %s", (user_id,))
        user = cursor.fetchone()
        
        # ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ããªã„
        if user and user['role'] == 'employee':
            raise HTTPException(
                status_code=403,
                detail="ç¤¾å“¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
            )
            
        if user and user['company_id'] and not company_id:
            company_id = user['company_id']
    if not file.filename.endswith(('.xlsx', '.xls', '.pdf', '.txt', '.avi', '.mp4', '.webp')):
        raise HTTPException(
            status_code=400,
            detail="ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚Excelãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.xlsxã€.xlsã€.pdfã€.txtï¼‰ã®ã¿å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
        )

    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        remaining_uploads = None
        limit_reached = False
        
        if user_id:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®åˆ©ç”¨åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
            limits_check = check_usage_limits(user_id, "document_upload", db)
            
            if not limits_check["is_unlimited"] and not limits_check["allowed"]:
                raise HTTPException(
                    status_code=403,
                    detail=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¢ç‰ˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ¶é™ï¼ˆ{limits_check['limit']}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚"
                )
            
            # ç„¡åˆ¶é™ã§ãªã„å ´åˆã¯æ®‹ã‚Šå›æ•°ã‚’è¨ˆç®—
            if not limits_check["is_unlimited"]:
                remaining_uploads = limits_check["remaining"]
        
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {file.filename}")
        contents = await file.read()
        file_size_mb = len(contents) / (1024 * 1024)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
        
        if len(contents) == 0:
            print("ã‚¨ãƒ©ãƒ¼: ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«")
            raise HTTPException(
                status_code=400,
                detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™ã€‚æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            )
        
        # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯è­¦å‘Š
        if file_size_mb > 10:
            print(f"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ ({file_size_mb:.2f} MB)ã€‚å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")
            
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ãŸå‡¦ç†
        file_extension = file.filename.split('.')[-1].lower()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–
        df = None
        sections = {}
        extracted_text = ""
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ãŸå‡¦ç†é–¢æ•°ã‚’å‘¼ã³å‡ºã™
            if file_extension in ['xlsx', 'xls']:
                print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                df, sections, extracted_text = _process_excel_file(contents, file.filename)
                print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {len(df)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")
            elif file_extension == 'pdf':
                print(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                if file_size_mb > 10:
                    raise HTTPException(
                        status_code=400,
                        detail=f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.2f} MB)ã€‚10MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
                    )
                
                df, sections, extracted_text = await _process_pdf_file(contents, file.filename)
                print(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {len(df)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")
            elif file_extension == 'txt':
                print(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file.filename}")
                df, sections, extracted_text = _process_txt_file(contents, file.filename)
                print(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {len(df)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")

            elif file_extension in ['.avi', 'mp4', '.webm']:
                if file_size_mb > 500:
                    raise HTTPException(
                        status_code=400,
                        detail=f"ãƒ“ãƒ‡ã‚ªãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({file_size_mb:.2f} MB)ã€‚500MBä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"
                    )
                
                df, sections, extracted_text = _process_video_file(contents, file.filename)
                print(f"Videoãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {len(df)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º")
                
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†…å®¹ã‚’ç¢ºèª
            if df is not None and not df.empty:
                print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åˆ—: {df.columns.tolist()}")
                print(f"æœ€åˆã®è¡Œ: {df.iloc[0].to_dict() if len(df) > 0 else 'ãªã—'}")
            else:
                print("è­¦å‘Š: ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")
                
        except HTTPException:
            # HTTPExceptionã¯ãã®ã¾ã¾å†ã‚¹ãƒ­ãƒ¼
            raise
        except Exception as e:
            error_type = {
                'xlsx': 'Excel', 'xls': 'Excel',
                'pdf': 'PDF',
                'txt': 'ãƒ†ã‚­ã‚¹ãƒˆ'
            }.get(file_extension, 'ãƒ•ã‚¡ã‚¤ãƒ«')
            
            print(f"{error_type}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            print(traceback.format_exc())
            
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®ç‰¹åˆ¥å‡¦ç†
            if "timeout" in str(e).lower():
                raise HTTPException(
                    status_code=408,  # Request Timeout
                    detail=f"å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹ã‹ã€è¤‡é›‘ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
                )
            else:
                raise HTTPException(
                    status_code=500,
                    detail=f"{error_type}ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                )
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ï¼‰
        if df is not None and not df.empty:
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if 'file' not in df.columns:
                df['file'] = file.filename
                print(f"'file'åˆ—ãŒãªã„ãŸã‚è¿½åŠ ã—ã¾ã—ãŸ: {file.filename}")
                
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
            _update_knowledge_base(df, extracted_text, is_file=True, source_name=file.filename, company_id=company_id)
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ : {file.filename} (ä¼šç¤¾ID: {company_id})")
            
            # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®çŠ¶æ…‹ã‚’ç¢ºèª
            print(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚µã‚¤ã‚º: {len(knowledge_base.data) if knowledge_base.data is not None else 0} è¡Œ")
            print(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿æ•°: {len(knowledge_base.file_data)}")
        else:
            print("è­¦å‘Š: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç©ºã®ãŸã‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã¯æ›´æ–°ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ ã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨æœ‰åŠ¹çŠ¶æ…‹ã‚’è¨˜éŒ²
        if file.filename not in knowledge_base.sources:
            knowledge_base.sources.append(file.filename)
            # ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨˜éŒ²
            knowledge_base.source_info[file.filename] = {
                'timestamp': datetime.now().isoformat(),
                'active': True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹
            }
            print(f"ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ : {file.filename} (ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: True)")
        else:
            print(f"ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«æ—¢ã«å­˜åœ¨: {file.filename}")
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’ç¢ºèª
            active = knowledge_base.source_info.get(file.filename, {}).get('active', True)
            print(f"ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹: {active}")
            
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒã‚ã‚‹å ´åˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
        # if user_id and not limits_check.get("is_unlimited", False):
        if user_id:
            updated_limits = update_usage_count(user_id, "document_uploads_used", db)
            remaining_uploads = updated_limits["document_uploads_limit"] - updated_limits["document_uploads_used"]
            limit_reached = remaining_uploads <= 0
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚½ãƒ¼ã‚¹ã‚’è¨˜éŒ²
            document_id = str(uuid.uuid4())
            page_count = None
            if file_extension == 'pdf':
                try:
                    pdf_reader = PyPDF2.PdfReader(BytesIO(contents))
                    page_count = len(pdf_reader.pages)
                except:
                    pass
          
            cursor = db.cursor()

            cursor.execute(
                "INSERT INTO document_sources (id, name, type, page_count, content, uploaded_by, company_id, uploaded_at, active) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)",
                (document_id, file.filename, file_extension.upper(), page_count, extracted_text, user_id, company_id, datetime.now().isoformat(), True)
            )

            # ä¼šç¤¾ã®ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
            if company_id:
                if company_id not in knowledge_base.company_sources:
                    knowledge_base.company_sources[company_id] = []
                if file.filename not in knowledge_base.company_sources[company_id]:
                    knowledge_base.company_sources[company_id].append(file.filename)
            db.commit()
        
        # æœ€æ–°ã®ä¼šç¤¾åã‚’å–å¾—
        from .company import DEFAULT_COMPANY_NAME as current_company_name
        
        # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        preview_data = []
        total_rows = 0
        
        if df is not None and not df.empty:
            preview_data = df.head(5).to_dict('records')
            # NaNå€¤ã‚’é©åˆ‡ã«å‡¦ç†
            preview_data = [{k: (None if pd.isna(v) else v) for k, v in record.items()} for record in preview_data]
            total_rows = len(df)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ã‚’å–å¾—
        active_sources = get_active_resources()
        print(f"ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹: {active_sources}")
        
        return {
            "message": f"{current_company_name}ã®æƒ…å ±ãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸï¼ˆãƒ•ã‚¡ã‚¤ãƒ«: {file.filename}ï¼‰",
            "columns": knowledge_base.columns if knowledge_base.data is not None else [],
            "preview": preview_data,
            "total_rows": total_rows,
            "sections": list(sections.keys()),
            "file": file.filename,
            "sources": knowledge_base.sources,
            "active_sources": active_sources,
            "remaining_uploads": remaining_uploads,
            "limit_reached": limit_reached
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )
# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_active_resources(company_id=None):
    """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã®ã¿ã‚’å–å¾—ã™ã‚‹"""
    active_sources = []
    
    # ä¼šç¤¾IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ä¼šç¤¾ã®ãƒªã‚½ãƒ¼ã‚¹ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹
    if company_id and company_id in knowledge_base.company_sources:
        company_sources = knowledge_base.company_sources[company_id]
        for source in company_sources:
            if source in knowledge_base.source_info and knowledge_base.source_info[source].get('active', True):
                active_sources.append(source)
    else:
        # ä¼šç¤¾IDãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã™ã¹ã¦ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒªã‚½ãƒ¼ã‚¹ã‚’è¿”ã™
        for source in knowledge_base.sources:
            if source in knowledge_base.source_info and knowledge_base.source_info[source].get('active', True):
                active_sources.append(source)
    
    return active_sources

# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_knowledge_base_info():
    """ç¾åœ¨ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    # æœ€æ–°ã®ä¼šç¤¾åã‚’å–å¾—
    from .company import DEFAULT_COMPANY_NAME as current_company_name
    
    # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’æ•´å½¢
    sources_info = []
    for source in knowledge_base.sources:
        info = knowledge_base.source_info.get(source, {})
        source_type = "URL" if source.startswith(('http://', 'https://')) else "ãƒ•ã‚¡ã‚¤ãƒ«"
        
        sources_info.append({
            "name": source,
            "type": source_type,
            "timestamp": info.get('timestamp', 'ä¸æ˜'),
            "active": info.get('active', True)
        })
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚½ãƒ¼ã‚¹ã‚’å–å¾—
    active_sources = get_active_resources()
    
    return {
        "company_name": current_company_name,
        "total_sources": len(knowledge_base.sources),
        "active_sources": len(active_sources),
        "sources": sources_info,
        "data_size": len(knowledge_base.data) if knowledge_base.data is not None else 0,
        "columns": knowledge_base.columns if knowledge_base.data is not None else []
    }

# Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹å†…éƒ¨é–¢æ•°
def _process_excel_file(contents, filename):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        # BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        excel_file = BytesIO(contents)
        
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        df_dict = pd.read_excel(excel_file, sheet_name=None)
        
        # å…¨ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
        all_data = []
        sections = {}
        extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n"
        
        for sheet_name, sheet_df in df_dict.items():
            # ã‚·ãƒ¼ãƒˆåã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦è¿½åŠ 
            section_name = f"ã‚·ãƒ¼ãƒˆ: {sheet_name}"
            sections[section_name] = sheet_df.to_string(index=False)
            extracted_text += f"=== {section_name} ===\n{sheet_df.to_string(index=False)}\n\n"
            
            # å„è¡Œã®ã™ã¹ã¦ã®å†…å®¹ã‚’çµåˆã—ã¦ content åˆ—ã‚’ä½œæˆ
            for _, row in sheet_df.iterrows():
                row_dict = row.to_dict()
                
                # content åˆ—ã‚’ä½œæˆï¼ˆã™ã¹ã¦ã®åˆ—ã®å€¤ã‚’çµåˆï¼‰
                content_parts = []
                for col, val in row_dict.items():
                    if not pd.isna(val):  # NaNå€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        content_parts.append(f"{val}")
                
                # çµåˆã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¨­å®š
                row_dict['content'] = " ".join(str(part) for part in content_parts if part)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                row_dict['section'] = section_name
                row_dict['source'] = 'Excel'
                row_dict['file'] = filename
                row_dict['url'] = None
                all_data.append(row_dict)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        result_df = pd.DataFrame(all_data) if all_data else pd.DataFrame({
            'section': [], 'content': [], 'source': [], 'file': [], 'url': []
        })
        
        # å¿…é ˆåˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        for col in ['section', 'source', 'file', 'url', 'content']:
            if col not in result_df.columns:
                if col == 'source':
                    result_df[col] = 'Excel'
                elif col == 'file':
                    result_df[col] = filename
                elif col == 'content':
                    # å„è¡Œã®å…¨ã¦ã®åˆ—ã®å€¤ã‚’çµåˆã—ã¦ content åˆ—ã‚’ä½œæˆ
                    if not result_df.empty:
                        result_df[col] = result_df.apply(
                            lambda row: " ".join(str(val) for val in row.values if not pd.isna(val)),
                            axis=1
                        )
                else:
                    result_df[col] = None
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
        print(f"å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åˆ—: {result_df.columns.tolist()}")
        if not result_df.empty:
            print(f"æœ€åˆã®è¡Œã® content: {result_df['content'].iloc[0]}")
        
        return result_df, sections, extracted_text
    except Exception as e:
        print(f"Excelãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹å†…éƒ¨é–¢æ•°
async def _process_pdf_file(contents, filename):
    """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        # BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        pdf_file = BytesIO(contents)
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        all_text = ""
        sections = {}
        extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n"
        
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            if page_text:
                page_text = page_text.replace('\x00', '') # ğŸ§¼ Remove NUL characters
                section_name = f"ãƒšãƒ¼ã‚¸ {i+1}" 
                sections[section_name] = page_text
                all_text += page_text + "\n"
                extracted_text += f"=== {section_name} ===\n{page_text}\n\n"
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
        import re
        # è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³
        heading_pattern = r'^(?:\d+[\.\s]+|ç¬¬\d+[ç« ç¯€]\s+|[\*\#]+\s+)?([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,}[ï¼š:ã€ã€‚])'
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        all_data = []
        current_section = "ä¸€èˆ¬æƒ…å ±"
        current_content = []
        for line in all_text.split("\n"):
            line = line.strip()
            if not line:
                continue
            
            # è¦‹å‡ºã—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            if re.search(heading_pattern, line):
                # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                if current_content:
                    content_text = "\n".join(current_content)
                    all_data.append({
                        'section': current_section,
                        'content': content_text,
                        'source': 'PDF',
                        'file': filename,
                        'url': None
                    })
                
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
                current_section = line
                current_content = []
            else:
                current_content.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
        if current_content:
            content_text = "\n".join(current_content)
            all_data.append({
                'section': current_section,
                'content': content_text,
                'source': 'PDF',
                'file': filename,
                'url': None
            })
        
        if all_text == "":
            all_text = await ocr_pdf_to_text_from_bytes(contents)
            result_df = pd.DataFrame(all_data) if all_data else pd.DataFrame({
                'section': ["ä¸€èˆ¬æƒ…å ±"],
                'content': [all_text],
                'source': ['PDF'],
                'file': [filename],
                'url': [None]
            })
            extracted_text += all_text
        else: 
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
            result_df = pd.DataFrame(all_data) if all_data else pd.DataFrame({
                'section': ["ä¸€èˆ¬æƒ…å ±"],
                'content': [all_text],
                'source': ['PDF'],
                'file': [filename],
                'url': [None]
            })
        
        return result_df, sections, extracted_text
    except Exception as e:
        print(f"PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise

# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹å†…éƒ¨é–¢æ•°
def _process_txt_file(contents, filename):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        try:
            text = contents.decode('utf-8')
        except UnicodeDecodeError:
            try:
                text = contents.decode('shift-jis')
            except UnicodeDecodeError:
                text = contents.decode('latin-1')
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
        import re
        # è¦‹å‡ºã—ãƒ‘ã‚¿ãƒ¼ãƒ³
        heading_pattern = r'^(?:\d+[\.\s]+|ç¬¬\d+[ç« ç¯€]\s+|[\*\#]+\s+)?([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]{2,}[ï¼š:ã€ã€‚])'
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        all_data = []
        sections = {}
        extracted_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {filename} ===\n\n"
        
        current_section = "ä¸€èˆ¬æƒ…å ±"
        current_content = []
        
        for line in text.split("\n"):
            line = line.strip()
            if not line:
                continue
            
            # è¦‹å‡ºã—ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            if re.search(heading_pattern, line):
                # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
                if current_content:
                    content_text = "\n".join(current_content)
                    sections[current_section] = content_text
                    extracted_text += f"=== {current_section} ===\n{content_text}\n\n"
                    all_data.append({
                        'section': current_section,
                        'content': content_text,
                        'source': 'TXT',
                        'file': filename,
                        'url': None
                    })
                
                # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
                current_section = line
                current_content = []
            else:
                current_content.append(line)
        
        # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
        if current_content:
            content_text = "\n".join(current_content)
            sections[current_section] = content_text
            extracted_text += f"=== {current_section} ===\n{content_text}\n\n"
            all_data.append({
                'section': current_section,
                'content': content_text,
                'source': 'TXT',
                'file': filename,
                'url': None
            })
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        result_df = pd.DataFrame(all_data) if all_data else pd.DataFrame({
            'section': ["ä¸€èˆ¬æƒ…å ±"],
            'content': [text],
            'source': ['TXT'],
            'file': [filename],
            'url': [None]
        })
        
        return result_df, sections, extracted_text
    except Exception as e:
        print(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise
# çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹å†…éƒ¨é–¢æ•°
def _update_knowledge_base(df, text, is_file=True, source_name=None, company_id=None):
    """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ã™ã‚‹å†…éƒ¨é–¢æ•°"""
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if source_name:
        knowledge_base.original_data[source_name] = {
            'df': df.copy(),
            'text': text,
            'company_id': company_id
        }
        
        # ä¼šç¤¾ã®ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
        if company_id:
            if company_id not in knowledge_base.company_sources:
                knowledge_base.company_sources[company_id] = []
            if source_name not in knowledge_base.company_sources[company_id]:
                knowledge_base.company_sources[company_id].append(source_name)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‹URLã‹ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if is_file:
        knowledge_base.file_data.append(df)
        knowledge_base.file_texts.append(text)
    else:
        knowledge_base.url_data.append(df)
        knowledge_base.url_texts.append(text)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_data = []
    if knowledge_base.file_data:
        all_data.extend(knowledge_base.file_data)
    if knowledge_base.url_data:
        all_data.extend(knowledge_base.url_data)
    
    if all_data:
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ
        knowledge_base.data = pd.concat(all_data, ignore_index=True)
        
        # åˆ—åã‚’ä¿å­˜
        knowledge_base.columns = knowledge_base.data.columns.tolist()
        
        # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        all_texts = []
        if knowledge_base.file_texts:
            all_texts.extend(knowledge_base.file_texts)
        if knowledge_base.url_texts:
            all_texts.extend(knowledge_base.url_texts)
        
        knowledge_base.raw_text = "\n\n".join(all_texts)
    
    print(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†: {len(knowledge_base.data) if knowledge_base.data is not None else 0} è¡Œã®ãƒ‡ãƒ¼ã‚¿")

# ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹é–¢æ•°
async def toggle_resource_active(resource_name: str):
    """ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹"""
    if resource_name not in knowledge_base.sources:
        raise HTTPException(
            status_code=404,
            detail=f"ãƒªã‚½ãƒ¼ã‚¹ '{resource_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        )
    
    # ç¾åœ¨ã®çŠ¶æ…‹ã‚’å–å¾—
    current_state = knowledge_base.source_info.get(resource_name, {}).get('active', True)
    
    # çŠ¶æ…‹ã‚’åè»¢
    new_state = not current_state
    
    # çŠ¶æ…‹ã‚’æ›´æ–°
    if resource_name not in knowledge_base.source_info:
        knowledge_base.source_info[resource_name] = {}
    
    knowledge_base.source_info[resource_name]['active'] = new_state
    
    return {
        "name": resource_name,
        "active": new_state,
        "message": f"ãƒªã‚½ãƒ¼ã‚¹ '{resource_name}' ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ã‚’ {new_state} ã«å¤‰æ›´ã—ã¾ã—ãŸ"
    }

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹é–¢æ•°
async def get_uploaded_resources():
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ï¼ˆURLã€PDFã€Excelã€TXTï¼‰ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
    resources = []
    
    for source in knowledge_base.sources:
        info = knowledge_base.source_info.get(source, {})
        
        # ãƒªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        if source.startswith(('http://', 'https://')):
            resource_type = "URL"
        else:
            extension = source.split('.')[-1].lower() if '.' in source else ""
            if extension in ['xlsx', 'xls']:
                resource_type = "Excel"
            elif extension == 'pdf':
                resource_type = "PDF"
            elif extension == 'txt':
                resource_type = "ãƒ†ã‚­ã‚¹ãƒˆ"
            if extension in ['avi', 'mp4', 'webp']:
                resource_type = "Video"
            else:
                resource_type = "ãã®ä»–"
        
        resources.append({
            "name": source,
            "type": resource_type,
            "timestamp": info.get('timestamp', datetime.now().isoformat()),
            "active": info.get('active', True)
        })
    
    return {
        "resources": resources,
        "message": f"{len(resources)}ä»¶ã®ãƒªã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ"
    }

# using gemini ocr 
# def ocr_with_gemini(images, instruction):
#     batch_size = 4 # performance size
#     prompt = f"""
#     {instruction}
#     These are pages from a PDF document. Extract all text content while preserving the structure.
#     Pay special attention to tables, columns, headers, and any structured content.
#     Maintain paragraph breaks and formatting.
#     """
#     # Assuming `model.generate_content` works with images and a prompt
#     # response = model.generate_content([prompt, *images])  # Passing the image objects directly

#     # Combine all text parts if it's a multi-part response
#     full_text = ""
#     for i in range(0, len(images), batch_size):
#         image_batch = images[i:i+batch_size]
#         try:
#             response = model.generate_content([prompt, *image_batch])
#             for part in response.parts:
#                 full_text += part.text
#         except Exception as e:
#             full_text += f"\n\n[Error processing pages {i}â€“{i+batch_size-1}]: {e}\n"

#     return full_text
# async def ocr_with_gemini(images, instruction):
#     prompt_base = f"""
#     {instruction}
#     This is a page from a PDF document. Extract all text content while preserving the structure.
#     Pay special attention to tables, columns, headers, and any structured content.
#     Maintain paragraph breaks and formatting.
#     """
    
#     full_text = ""
#     for idx, image in enumerate(images):
#         try:
#             prompt = f"{prompt_base}\n\nPage {idx + 1}:"
#             response = model.generate_content([prompt, image])
#             for part in response.parts:
#                 full_text += f"\n\n--- Page {idx + 1} ---\n{part.text}"
#         except Exception as e:
#             full_text += f"\n\n[Error processing page {idx + 1}]: {e}\n"

#     return full_text
async def ocr_with_gemini(images, instruction):
    prompt_base = f"""
    {instruction}
    This is a page from a PDF document. Extract all text content while preserving the structure.
    Pay special attention to tables, columns, headers, and any structured content.
    Maintain paragraph breaks and formatting.
    """

    async def process_page(idx, image):
        def sync_call():
            prompt = f"{prompt_base}\n\nPage {idx + 1}:"
            response = model.generate_content([prompt, image])
            return f"\n\n--- Page {idx + 1} ---\n" + "".join(part.text for part in response.parts)

        try:
            return await asyncio.to_thread(sync_call)
        except Exception as e:
            return f"\n\n[Error processing page {idx + 1}]: {e}\n"

    tasks = [process_page(idx, img) for idx, img in enumerate(images)]
    results = await asyncio.gather(*tasks)

    return "".join(results)

# Main OCR function
async def ocr_pdf_to_text_from_bytes(pdf_content: bytes):
    # Convert PDF to images directly from bytes
    images = convert_pdf_to_images_from_bytes(pdf_content)

    # Define instruction for Gemini OCR
    instruction = """
    Extract ALL text content from these document pages.
    For tables:
    1. Maintain the table structure using markdown table format.
    2. Preserve all column headers and row labels.
    3. Ensure numerical data is accurately captured.
    For multi-column layouts:
    1. Process columns from left to right.
    2. Clearly separate content from different columns.
    For charts and graphs:
    1. Describe the chart type.
    2. Extract any visible axis labels, legends, and data points.
    3. Extract any title or caption.
    Preserve all headers, footers, page numbers, and footnotes.
    """

    # Extract text using Gemini OCR
    extracted_text = await ocr_with_gemini(images, instruction)

    return extracted_text

# Convert PDF to images directly from bytes
def convert_pdf_to_images_from_bytes(pdf_content, dpi=200):
    images = convert_from_bytes(pdf_content, dpi=dpi)
    return images  # This will return a list of PIL Image objects
# ä»¥ä¸‹ã¯æ—¢å­˜ã®å‡¦ç†é–¢æ•°ï¼ˆ_process_excel_file, _process_pdf_file, _process_txt_file, _extract_text_from_image_with_geminiï¼‰
# ã“ã‚Œã‚‰ã®é–¢æ•°ã¯å¤‰æ›´ã›ãšã€ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™